{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad1c1bf",
   "metadata": {},
   "source": [
    "# üêï DetectoDog: Dog Breed Classification\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "DetectoDog is a deep learning model that accurately identifies dog breeds from images. This project demonstrates the complete machine learning workflow from data preparation to model deployment, using transfer learning with state-of-the-art convolutional neural networks.\n",
    "\n",
    "### Dataset\n",
    "The Stanford Dogs Dataset contains images of 120 breeds of dogs from around the world. The dataset includes:\n",
    "- 20,580 total images\n",
    "- 120 different dog breeds\n",
    "- Approximately 150-200 images per breed\n",
    "- Varying image quality, angles, and backgrounds\n",
    "\n",
    "### Approach\n",
    "This project uses transfer learning with three different backbone architectures:\n",
    "- **ResNet50**: Deep residual network with excellent feature extraction capabilities\n",
    "- **MobileNetV2**: Lightweight architecture optimized for mobile devices\n",
    "- **EfficientNet**: State-of-the-art model with optimal accuracy/efficiency tradeoff\n",
    "\n",
    "### Key Features\n",
    "- ‚úÖ Comprehensive data preprocessing and augmentation\n",
    "- ‚úÖ Transfer learning with multiple CNN architectures\n",
    "- ‚úÖ Hyperparameter optimization strategies\n",
    "- ‚úÖ Performance comparison across architectures\n",
    "- ‚úÖ Detailed visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e70f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import tarfile\n",
    "import scipy.io\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"üßÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üß† Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Display PyTorch and Torchvision versions\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üì¶ Torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "experiments_dir = \"experiments\"\n",
    "os.makedirs(experiments_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Experiments directory ready: {experiments_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d560e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class for managing training and model parameters\n",
    "    \n",
    "    This class centralizes all configuration parameters and provides\n",
    "    methods for saving/loading configurations.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize configuration with default values or from a dictionary\n",
    "        \n",
    "        Args:\n",
    "            config_dict: Optional dictionary with configuration values\n",
    "        \"\"\"\n",
    "        # Set default values\n",
    "        self.set_defaults()\n",
    "        \n",
    "        # Update with provided dictionary if any\n",
    "        if config_dict:\n",
    "            for key, value in config_dict.items():\n",
    "                setattr(self, key, value)\n",
    "        \n",
    "        # Automatically check for GPU\n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_gpu else \"cpu\")\n",
    "        \n",
    "        # Derive experiment name if not set\n",
    "        if not hasattr(self, 'experiment_name') or not self.experiment_name:\n",
    "            self.experiment_name = f\"{self.backbone}_{self.lr_strategy}_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "        \n",
    "        # Apply device-specific adjustments\n",
    "        self._adjust_for_device()\n",
    "    \n",
    "    def set_defaults(self):\n",
    "        \"\"\"Set default configuration values\"\"\"\n",
    "        # General settings\n",
    "        self.experiment_name = None\n",
    "        self.device = \"cpu\"\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Model configuration\n",
    "        self.backbone = \"efficientnet\"  # Options: \"resnet50\", \"mobilenetv2\", \"efficientnet\"\n",
    "        self.dropout_rate = 0.3\n",
    "        self.freeze_backbone = False\n",
    "        \n",
    "        # Dataset configuration\n",
    "        self.image_size = (224, 224)\n",
    "        self.use_augmentation = True\n",
    "        \n",
    "        # Training configuration\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 25\n",
    "        self.lr_strategy = \"cyclic\"  # Options: \"cyclic\", \"fixed\"\n",
    "        self.fixed_lr = 0.0001\n",
    "        self.cyclic_base_lr = 0.00001\n",
    "        self.cyclic_max_lr = 0.00005\n",
    "        self.cyclic_step_size = 340  # = 5 epochs * 68 batches per epoch\n",
    "        \n",
    "        # Paths\n",
    "        self.experiments_dir = \"experiments\"  # Added experiments directory\n",
    "        self.dataset_path = \"dataset/Images\"\n",
    "        self.log_dir = \"logs\"\n",
    "        self.checkpoint_dir = \"checkpoints\"\n",
    "        self.experiment_csv = os.path.join(self.experiments_dir, \"training_runs_v4.csv\")  # Updated path\n",
    "    \n",
    "    def _adjust_for_device(self):\n",
    "        \"\"\"Adjust parameters based on the device (CPU/GPU)\"\"\"\n",
    "        if self.device.type == \"cpu\":\n",
    "            # Reduce workload for CPU training\n",
    "            self.batch_size = min(8, self.batch_size)\n",
    "            self.num_epochs = min(3, self.num_epochs)\n",
    "            \n",
    "            # Use smaller images for faster processing\n",
    "            if self.backbone == \"efficientnet\":\n",
    "                self.image_size = (128, 128)\n",
    "            else:\n",
    "                self.image_size = (128, 128)\n",
    "            \n",
    "            print(\"‚ö†Ô∏è Running on CPU ‚Äì Using smaller dataset and fewer epochs for faster training.\")\n",
    "        else:\n",
    "            # Optimize for GPU\n",
    "            if self.backbone == \"efficientnet\":\n",
    "                self.image_size = (240, 240)  # EfficientNet prefers larger images\n",
    "            \n",
    "            print(f\"‚úÖ Running on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the configuration to a file\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to save the configuration\n",
    "        \"\"\"\n",
    "        # Convert to dictionary\n",
    "        config_dict = {k: v for k, v in self.__dict__.items() \n",
    "                      if not k.startswith('_') and not callable(v)}\n",
    "        \n",
    "        # Handle non-serializable types\n",
    "        if 'device' in config_dict:\n",
    "            config_dict['device'] = str(config_dict['device'])\n",
    "        \n",
    "        # Save based on file extension\n",
    "        ext = os.path.splitext(filepath)[1].lower()\n",
    "        \n",
    "        if ext == '.yaml' or ext == '.yml':\n",
    "            with open(filepath, 'w') as f:\n",
    "                yaml.dump(config_dict, f, default_flow_style=False)\n",
    "        elif ext == '.json':\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(config_dict, f, indent=4)\n",
    "        else:\n",
    "            # Default to JSON\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(config_dict, f, indent=4)\n",
    "        \n",
    "        print(f\"‚úÖ Configuration saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load configuration from a file\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the configuration file\n",
    "            \n",
    "        Returns:\n",
    "            Config: Configuration object\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(filepath)[1].lower()\n",
    "        \n",
    "        if ext == '.yaml' or ext == '.yml':\n",
    "            with open(filepath, 'r') as f:\n",
    "                config_dict = yaml.safe_load(f)\n",
    "        elif ext == '.json':\n",
    "            with open(filepath, 'r') as f:\n",
    "                config_dict = json.load(f)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "        \n",
    "        return cls(config_dict)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Return a string representation of the configuration\"\"\"\n",
    "        lines = [\"üìã CONFIGURATION:\"]\n",
    "        lines.append(\"=\" * 50)\n",
    "        \n",
    "        # General settings\n",
    "        lines.append(\"üìå General Settings:\")\n",
    "        lines.append(f\"  ‚Ä¢ Experiment Name: {self.experiment_name}\")\n",
    "        lines.append(f\"  ‚Ä¢ Device: {self.device}\")\n",
    "        lines.append(f\"  ‚Ä¢ Random Seed: {self.seed}\")\n",
    "        \n",
    "        # Model configuration\n",
    "        lines.append(\"\\nüìå Model Configuration:\")\n",
    "        lines.append(f\"  ‚Ä¢ Backbone: {self.backbone}\")\n",
    "        lines.append(f\"  ‚Ä¢ Dropout Rate: {self.dropout_rate}\")\n",
    "        lines.append(f\"  ‚Ä¢ Freeze Backbone: {self.freeze_backbone}\")\n",
    "        \n",
    "        # Dataset configuration\n",
    "        lines.append(\"\\nüìå Dataset Configuration:\")\n",
    "        lines.append(f\"  ‚Ä¢ Image Size: {self.image_size}\")\n",
    "        lines.append(f\"  ‚Ä¢ Data Augmentation: {self.use_augmentation}\")\n",
    "        \n",
    "        # Training configuration\n",
    "        lines.append(\"\\nüìå Training Configuration:\")\n",
    "        lines.append(f\"  ‚Ä¢ Batch Size: {self.batch_size}\")\n",
    "        lines.append(f\"  ‚Ä¢ Num Epochs: {self.num_epochs}\")\n",
    "        lines.append(f\"  ‚Ä¢ Learning Rate Strategy: {self.lr_strategy}\")\n",
    "        \n",
    "        if self.lr_strategy == \"fixed\":\n",
    "            lines.append(f\"  ‚Ä¢ Learning Rate: {self.fixed_lr}\")\n",
    "        else:\n",
    "            lines.append(f\"  ‚Ä¢ Cyclic Base LR: {self.cyclic_base_lr}\")\n",
    "            lines.append(f\"  ‚Ä¢ Cyclic Max LR: {self.cyclic_max_lr}\")\n",
    "            lines.append(f\"  ‚Ä¢ Cyclic Step Size: {self.cyclic_step_size}\")\n",
    "        \n",
    "        # Paths\n",
    "        lines.append(\"\\nüìå Paths:\")\n",
    "        lines.append(f\"  ‚Ä¢ Dataset Path: {self.dataset_path}\")\n",
    "        lines.append(f\"  ‚Ä¢ Log Directory: {self.log_dir}\")\n",
    "        lines.append(f\"  ‚Ä¢ Checkpoint Directory: {self.checkpoint_dir}\")\n",
    "        lines.append(f\"  ‚Ä¢ Experiment CSV: {self.experiment_csv}\")\n",
    "        \n",
    "        lines.append(\"=\" * 50)\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75065255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for this experiment\n",
    "config = Config({\n",
    "    \"experiment_name\": \"detectodog_improved\",\n",
    "    \"backbone\": \"efficientnet\",  # Options: \"resnet50\", \"mobilenetv2\", \"efficientnet\"\n",
    "    \"experiment_csv\": \"training_runs_v4.csv\",\n",
    "    \"lr_strategy\": \"cyclic\",\n",
    "    \"use_augmentation\": True,\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"experiments_dir\": \"experiments\",\n",
    "    \"experiment_csv\": \"experiments/training_runs_v4.csv\"\n",
    "})\n",
    "\n",
    "# Display configuration\n",
    "print(config)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2f279",
   "metadata": {},
   "source": [
    "\n",
    "## üì• Dataset Download and Extraction\n",
    "\n",
    "We'll download the Stanford Dogs Dataset which contains images of 120 dog breeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Extract Dataset (can be skipped if dataset already exists)\n",
    "\n",
    "def download_and_extract_dataset():\n",
    "    \"\"\"Download and extract the Stanford Dogs Dataset\"\"\"\n",
    "    \n",
    "    # Set paths\n",
    "    dataset_path = \"images.tar\"\n",
    "    list_path = \"lists.tar\"\n",
    "    \n",
    "    # Download dataset\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Downloading dataset images (this may take a while)...\")\n",
    "        os.system(f\"wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Dataset images already downloaded: {dataset_path}\")\n",
    "    \n",
    "    # Download lists\n",
    "    if not os.path.exists(list_path):\n",
    "        print(\"Downloading dataset lists...\")\n",
    "        os.system(f\"wget http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Dataset lists already downloaded: {list_path}\")\n",
    "    \n",
    "    # Extract images\n",
    "    if not os.path.exists(\"dataset/Images\"):\n",
    "        print(\"Extracting images...\")\n",
    "        os.makedirs(\"dataset\", exist_ok=True)\n",
    "        with tarfile.open(dataset_path, \"r\") as tar:\n",
    "            tar.extractall(\"dataset/\")\n",
    "        print(\"‚úÖ Images extracted successfully\")\n",
    "    else:\n",
    "        print(\"‚úÖ Images already extracted\")\n",
    "    \n",
    "    # Extract lists\n",
    "    if not os.path.exists(\"dataset/train_list.mat\"):\n",
    "        print(\"Extracting train/test lists...\")\n",
    "        with tarfile.open(list_path, \"r\") as tar:\n",
    "            tar.extractall(\"dataset/\")\n",
    "        print(\"‚úÖ Lists extracted successfully\")\n",
    "    else:\n",
    "        print(\"‚úÖ Lists already extracted\")\n",
    "    \n",
    "    return \"dataset/Images\", \"dataset/train_list.mat\", \"dataset/test_list.mat\"\n",
    "\n",
    "# Download and extract if needed\n",
    "image_dir, train_list_path, test_list_path = download_and_extract_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c417ba",
   "metadata": {},
   "source": [
    "## üîç Dataset Exploration\n",
    "\n",
    "Let's explore the dataset to understand its structure, class distribution, and view some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Exploration\n",
    "def load_train_test_lists(train_path, test_path):\n",
    "    \"\"\"Load train and test lists from .mat files\"\"\"\n",
    "    \n",
    "    # Load .mat files\n",
    "    train_data = scipy.io.loadmat(train_path)\n",
    "    test_data = scipy.io.loadmat(test_path)\n",
    "    \n",
    "    # Print keys\n",
    "    print(\"Train data keys:\", train_data.keys())\n",
    "    print(\"Test data keys:\", test_data.keys())\n",
    "    \n",
    "    # Extract file lists\n",
    "    train_files = [str(file[0]) for file in train_data[\"file_list\"]]\n",
    "    test_files = [str(file[0]) for file in test_data[\"file_list\"]]\n",
    "    \n",
    "    # Clean up file paths (remove extra quotes and brackets)\n",
    "    train_files = [path.strip(\"[]'\") for path in train_files]\n",
    "    test_files = [path.strip(\"[]'\") for path in test_files]\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(train_files)} training files and {len(test_files)} test files\")\n",
    "    \n",
    "    return train_files, test_files\n",
    "\n",
    "def explore_dataset(image_dir, file_list):\n",
    "    \"\"\"Explore the dataset and show statistics\"\"\"\n",
    "    \n",
    "    # Get breed names\n",
    "    breeds = [path.split('/')[0] for path in file_list]\n",
    "    breed_counts = pd.Series(breeds).value_counts()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total images: {len(file_list)}\")\n",
    "    print(f\"Number of breeds: {len(breed_counts)}\")\n",
    "    print(f\"Average images per breed: {breed_counts.mean():.1f}\")\n",
    "    print(f\"Min images for a breed: {breed_counts.min()} ({breed_counts.idxmin()})\")\n",
    "    print(f\"Max images for a breed: {breed_counts.max()} ({breed_counts.idxmax()})\")\n",
    "    \n",
    "    # Plot breed distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_breeds = breed_counts.head(20)\n",
    "    sns.barplot(x=top_breeds.index, y=top_breeds.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Distribution of Top 20 Dog Breeds\")\n",
    "    plt.xlabel(\"Breed\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return breed_counts\n",
    "\n",
    "def show_sample_images(image_dir, file_list, num_samples=5):\n",
    "    \"\"\"Show sample images from different breeds\"\"\"\n",
    "    \n",
    "    # Get unique breeds\n",
    "    breeds = list(set([path.split('/')[0] for path in file_list]))\n",
    "    \n",
    "    # Sample some breeds\n",
    "    sampled_breeds = np.random.choice(breeds, min(num_samples, len(breeds)), replace=False)\n",
    "    \n",
    "    # Create a figure\n",
    "    fig, axes = plt.subplots(1, len(sampled_breeds), figsize=(15, 5))\n",
    "    \n",
    "    for i, breed in enumerate(sampled_breeds):\n",
    "        # Get all images for this breed\n",
    "        breed_files = [f for f in file_list if f.startswith(breed)]\n",
    "        \n",
    "        # Pick a random image\n",
    "        img_path = os.path.join(image_dir, np.random.choice(breed_files))\n",
    "        \n",
    "        # Load and display\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Clean up breed name for display\n",
    "        breed_name = breed.split('-')[1] if '-' in breed else breed\n",
    "        axes[i].set_title(breed_name)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load train and test lists\n",
    "train_files, test_files = load_train_test_lists(train_list_path, test_list_path)\n",
    "\n",
    "# If running on CPU with limited resources, use a subset\n",
    "if device.type == \"cpu\":\n",
    "    print(\"‚ö†Ô∏è Running on CPU, using subset of data for quicker processing\")\n",
    "    train_files = train_files[:500]\n",
    "    test_files = test_files[:100]\n",
    "\n",
    "# Explore dataset\n",
    "breed_counts = explore_dataset(image_dir, train_files)\n",
    "\n",
    "# Show sample images\n",
    "show_sample_images(image_dir, train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Dataset Class Definition\n",
    "class DogBreedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading dog breed images\n",
    "    \n",
    "    This dataset handles:\n",
    "    - Loading images from the filesystem\n",
    "    - Converting to RGB\n",
    "    - Applying transformations\n",
    "    - Converting breed names to numeric labels\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list, image_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            file_list: List of image file paths (relative to image_folder)\n",
    "            image_folder: Root folder containing all images\n",
    "            transform: PyTorch transforms to apply to images\n",
    "        \"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract all unique breed names\n",
    "        self.breeds = sorted(set([path.split('/')[0] for path in file_list]))\n",
    "        self.breed_to_idx = {breed: idx for idx, breed in enumerate(self.breeds)}\n",
    "        \n",
    "        # Check if any files are missing\n",
    "        self._validate_files()\n",
    "    \n",
    "    def _validate_files(self):\n",
    "        \"\"\"Check if all files in the list exist\"\"\"\n",
    "        missing_files = 0\n",
    "        for file_path in self.file_list[:100]:  # Check first 100 files only\n",
    "            full_path = os.path.join(self.image_folder, file_path)\n",
    "            if not os.path.exists(full_path):\n",
    "                missing_files += 1\n",
    "        \n",
    "        if missing_files > 0:\n",
    "            print(f\"‚ö†Ô∏è Warning: {missing_files} files not found in the first 100 files checked.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of images in the dataset\"\"\"\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an image and its label by index\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the item to get\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image_tensor, label_index)\n",
    "        \"\"\"\n",
    "        # Get file path\n",
    "        file_path = self.file_list[idx]\n",
    "        full_path = os.path.join(self.image_folder, file_path)\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(full_path).convert(\"RGB\")\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            # Get label\n",
    "            breed = file_path.split('/')[0]\n",
    "            label = self.breed_to_idx[breed]\n",
    "            \n",
    "            return image, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading {full_path}: {str(e)}\")\n",
    "            # Return a placeholder\n",
    "            if self.transform:\n",
    "                placeholder = torch.zeros((3, 224, 224))\n",
    "                return placeholder, 0\n",
    "            else:\n",
    "                placeholder = Image.new('RGB', (224, 224), color='gray')\n",
    "                return placeholder, 0\n",
    "    \n",
    "    def get_breed_counts(self):\n",
    "        \"\"\"Get the count of images for each breed\"\"\"\n",
    "        breed_counts = {}\n",
    "        for file_path in self.file_list:\n",
    "            breed = file_path.split('/')[0]\n",
    "            if breed in breed_counts:\n",
    "                breed_counts[breed] += 1\n",
    "            else:\n",
    "                breed_counts[breed] = 1\n",
    "        \n",
    "        return breed_counts\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_breed_name(breed):\n",
    "        \"\"\"Format a breed name for display (remove prefix, replace hyphens)\"\"\"\n",
    "        if '-' in breed:\n",
    "            # Remove prefix like 'n02085620-'\n",
    "            parts = breed.split('-')\n",
    "            return parts[1].replace('_', ' ').title()\n",
    "        else:\n",
    "            return breed.replace('_', ' ').title()\n",
    "    \n",
    "    def get_formatted_breeds(self):\n",
    "        \"\"\"Get a list of formatted breed names\"\"\"\n",
    "        return [self.format_breed_name(breed) for breed in self.breeds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a8d1f",
   "metadata": {},
   "source": [
    "## üîÑ Data Processing\n",
    "\n",
    "Now we'll define the transformations for our images and create our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transforms and Create Datasets\n",
    "def create_transforms(image_size, use_augmentation):\n",
    "    \"\"\"\n",
    "    Create transformation pipelines for training and validation\n",
    "    \n",
    "    Args:\n",
    "        image_size: Image size (tuple of height, width)\n",
    "        use_augmentation: Whether to use data augmentation\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_transform, valid_transform)\n",
    "    \"\"\"\n",
    "    # Always apply the same validation transform\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Training transform with or without augmentation\n",
    "    if use_augmentation:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((int(image_size[0] * 1.1), int(image_size[1] * 1.1))),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        # No augmentation, use same as validation\n",
    "        train_transform = valid_transform\n",
    "    \n",
    "    return train_transform, valid_transform\n",
    "\n",
    "def show_augmentations(image_path, transform, num_samples=5):\n",
    "    \"\"\"Show multiple augmented versions of the same image\"\"\"\n",
    "    # Load image\n",
    "    original_img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Create augmented versions\n",
    "    augmented_imgs = []\n",
    "    for _ in range(num_samples):\n",
    "        # Apply transform\n",
    "        augmented_img = transform(original_img)\n",
    "        \n",
    "        # Convert tensor to numpy for visualization\n",
    "        img_np = augmented_img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Denormalize\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_np = std * img_np + mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        \n",
    "        augmented_imgs.append(img_np)\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, num_samples + 1, figsize=(15, 5))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Augmented\n",
    "    for i, img_np in enumerate(augmented_imgs):\n",
    "        axes[i+1].imshow(img_np)\n",
    "        axes[i+1].set_title(f\"Augmentation {i+1}\")\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create transforms\n",
    "train_transform, valid_transform = create_transforms(\n",
    "    image_size=config.image_size,\n",
    "    use_augmentation=config.use_augmentation\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DogBreedDataset(train_files, image_dir, transform=train_transform)\n",
    "test_dataset = DogBreedDataset(test_files, image_dir, transform=valid_transform)\n",
    "\n",
    "print(f\"‚úÖ Created datasets with {len(train_dataset)} training samples and {len(test_dataset)} test samples\")\n",
    "\n",
    "# Get breed labels\n",
    "breed_labels = train_dataset.breeds\n",
    "print(f\"‚úÖ Using {len(breed_labels)} breed classes\")\n",
    "\n",
    "# Show data augmentations (if images are available)\n",
    "try:\n",
    "    # Get a sample image path\n",
    "    sample_file = train_files[0]\n",
    "    sample_path = os.path.join(image_dir, sample_file)\n",
    "    \n",
    "    # Show augmentations\n",
    "    show_augmentations(sample_path, train_transform)\n",
    "except Exception as e:\n",
    "    print(f\"Could not show augmentations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4 if config.device.type == \"cuda\" else 0,\n",
    "    pin_memory=True if config.device.type == \"cuda\" else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4 if config.device.type == \"cuda\" else 0,\n",
    "    pin_memory=True if config.device.type == \"cuda\" else False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created DataLoaders with batch size {config.batch_size}\")\n",
    "print(f\"‚úÖ Training will use {len(train_loader)} batches per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c39d8",
   "metadata": {},
   "source": [
    "## üß† Model Architecture\n",
    "\n",
    "Now we'll define our model architecture using transfer learning with pretrained backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73de0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class DogBreedClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Dog breed classifier using transfer learning with various backbone options.\n",
    "    \n",
    "    This model allows for multiple backbone architectures and customization\n",
    "    options for fine-tuning and feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, backbone=\"efficientnet\", dropout_rate=0.3, freeze_backbone=False):\n",
    "        \"\"\"\n",
    "        Initialize the model with the specified backbone and parameters.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Number of dog breed classes to classify\n",
    "            backbone (str): Which backbone architecture to use [\"resnet50\", \"mobilenetv2\", \"efficientnet\"]\n",
    "            dropout_rate (float): Dropout rate for regularization (0-1)\n",
    "            freeze_backbone (bool): Whether to freeze the backbone weights during training\n",
    "        \"\"\"\n",
    "        super(DogBreedClassifier, self).__init__()\n",
    "        \n",
    "        self.backbone_name = backbone\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Initialize the backbone architecture\n",
    "        if backbone == \"resnet50\":\n",
    "            self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "            self.feature_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()  # Remove classifier\n",
    "            \n",
    "        elif backbone == \"mobilenetv2\":\n",
    "            self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "            self.feature_dim = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()  # Remove classifier\n",
    "            \n",
    "        elif backbone == \"efficientnet\":\n",
    "            self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "            self.feature_dim = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()  # Remove classifier\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}. Choose from: resnet50, mobilenetv2, efficientnet\")\n",
    "        \n",
    "        # Freeze backbone if specified\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Custom classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate/2),  # Lower dropout in final layers\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "    def get_model_size(self):\n",
    "        \"\"\"Calculate model size in MB\"\"\"\n",
    "        param_size = 0\n",
    "        for param in self.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in self.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        return (param_size + buffer_size) / (1024 * 1024)\n",
    "    \n",
    "    def get_parameter_count(self):\n",
    "        \"\"\"Return the total number of parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "# Create model\n",
    "model = DogBreedClassifier(\n",
    "    num_classes=len(breed_labels),\n",
    "    backbone=config.backbone,\n",
    "    dropout_rate=config.dropout_rate,\n",
    "    freeze_backbone=config.freeze_backbone\n",
    ")\n",
    "model = model.to(config.device)\n",
    "\n",
    "# Print model info\n",
    "print(f\"üìä Model Architecture: {config.backbone}\")\n",
    "print(f\"üìä Total Parameters: {model.get_parameter_count():,}\")\n",
    "print(f\"üìä Model Size: {model.get_model_size():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3c770",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è Training Utilities\n",
    "\n",
    "Let's define the utilities for training, logging, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training and Experiment Utilities\n",
    "\n",
    "# Markdown cell:\n",
    "\"\"\"\n",
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è Training and Experiment Utilities\n",
    "\n",
    "Let's define the utilities for training, evaluation, and experiment tracking.\n",
    "\"\"\"\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"\n",
    "    Logger for tracking and visualizing training progress.\n",
    "    \n",
    "    This class handles:\n",
    "    - Per-epoch metrics tracking\n",
    "    - CSV logging\n",
    "    - Training progress visualization\n",
    "    - Model checkpointing\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir=\"logs\"):\n",
    "        \"\"\"\n",
    "        Initialize the training logger.\n",
    "        \n",
    "        Args:\n",
    "            log_dir: Directory to save logs and checkpoints\n",
    "        \"\"\"\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(log_dir, \"checkpoints\"), exist_ok=True)\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        self.run_id = int(datetime.now().timestamp())\n",
    "        self.start_time = time.time()\n",
    "        self.epoch_times = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "        # Create log CSV files with headers\n",
    "        self.metrics_file = os.path.join(log_dir, f\"metrics_{self.run_id}.csv\")\n",
    "        with open(self.metrics_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['epoch', 'train_loss', 'val_loss', 'val_accuracy', 'learning_rate', 'time_seconds'])\n",
    "    \n",
    "    def log_epoch(self, epoch, train_loss, val_loss, val_accuracy, learning_rate, epoch_time):\n",
    "        \"\"\"Log metrics for a completed epoch\"\"\"\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_accuracy)\n",
    "        self.learning_rates.append(learning_rate)\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Append to CSV\n",
    "        with open(self.metrics_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch, train_loss, val_loss, val_accuracy, learning_rate, epoch_time])\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, epoch, val_accuracy):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_path = os.path.join(self.log_dir, \"checkpoints\", f\"epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'train_loss': self.train_losses[-1] if self.train_losses else None,\n",
    "            'val_loss': self.val_losses[-1] if self.val_losses else None,\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Save best model separately\n",
    "        if val_accuracy == max(self.val_accuracies):\n",
    "            best_path = os.path.join(self.log_dir, \"best_model.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"‚úÖ New best model saved! Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    def plot_metrics(self, figsize=(15, 10)):\n",
    "        \"\"\"Plot training metrics\"\"\"\n",
    "        if not self.train_losses:\n",
    "            print(\"No training data to plot yet.\")\n",
    "            return\n",
    "            \n",
    "        epochs = list(range(1, len(self.train_losses) + 1))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        if self.val_losses:\n",
    "            axes[0, 0].plot(epochs, self.val_losses, 'r-', label='Validation Loss')\n",
    "        axes[0, 0].set_title('Loss vs. Epochs')\n",
    "        axes[0, 0].set_xlabel('Epochs')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot validation accuracy\n",
    "        if self.val_accuracies:\n",
    "            axes[0, 1].plot(epochs, self.val_accuracies, 'g-')\n",
    "            axes[0, 1].set_title('Validation Accuracy vs. Epochs')\n",
    "            axes[0, 1].set_xlabel('Epochs')\n",
    "            axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "            axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot learning rate\n",
    "        axes[1, 0].plot(epochs, self.learning_rates, 'm-')\n",
    "        axes[1, 0].set_title('Learning Rate vs. Epochs')\n",
    "        axes[1, 0].set_xlabel('Epochs')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot epoch times\n",
    "        axes[1, 1].plot(epochs, self.epoch_times, 'c-')\n",
    "        axes[1, 1].set_title('Epoch Time vs. Epochs')\n",
    "        axes[1, 1].set_xlabel('Epochs')\n",
    "        axes[1, 1].set_ylabel('Time (seconds)')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.log_dir, f\"training_metrics_{self.run_id}.png\"))\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, true_labels, predictions, class_names, figsize=(12, 10)):\n",
    "        \"\"\"Plot confusion matrix of model predictions\"\"\"\n",
    "        cm = confusion_matrix(true_labels, predictions)\n",
    "        \n",
    "        # Normalize the confusion matrix\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(cm_norm, annot=False, cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Normalized Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.log_dir, f\"confusion_matrix_{self.run_id}.png\"))\n",
    "        plt.show()\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print training summary\"\"\"\n",
    "        if not self.train_losses:\n",
    "            print(\"No training has been logged yet.\")\n",
    "            return\n",
    "            \n",
    "        total_time = time.time() - self.start_time\n",
    "        total_epochs = len(self.train_losses)\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìä TRAINING SUMMARY (Run ID: {self.run_id})\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total training time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"Total epochs: {total_epochs}\")\n",
    "        print(f\"Average epoch time: {sum(self.epoch_times)/len(self.epoch_times):.2f} seconds\")\n",
    "        print(f\"Best validation accuracy: {max(self.val_accuracies):.2f}%\")\n",
    "        print(f\"Final validation accuracy: {self.val_accuracies[-1]:.2f}%\")\n",
    "        print(f\"Final training loss: {self.train_losses[-1]:.4f}\")\n",
    "        print(f\"Final validation loss: {self.val_losses[-1]:.4f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    def log_epoch(self, epoch, train_loss, val_loss, val_accuracy, learning_rate, epoch_time):\n",
    "        \"\"\"Log metrics for a completed epoch\"\"\"\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_accuracy)\n",
    "        self.learning_rates.append(learning_rate)\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Append to CSV\n",
    "        with open(self.metrics_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch, train_loss, val_loss, val_accuracy, learning_rate, epoch_time])\n",
    "        \n",
    "        # ADDED: Log to epoch_loss.csv for backwards compatibility\n",
    "        self.log_to_epoch_loss_csv(epoch, train_loss)\n",
    "    \n",
    "    def log_to_epoch_loss_csv(self, epoch, loss):\n",
    "        \"\"\"Log epoch loss to the legacy epoch_loss.csv file\"\"\"\n",
    "        epoch_loss_file = os.path.join(config.experiments_dir, \"epoch_loss.csv\")\n",
    "        file_exists = os.path.isfile(epoch_loss_file)\n",
    "        \n",
    "        with open(epoch_loss_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if not file_exists:\n",
    "                writer.writerow(['run_id', 'epoch', 'loss'])\n",
    "            writer.writerow([self.run_id, epoch, loss])\n",
    "\n",
    "def log_run_to_csv(config, model, accuracy, train_time, logger, csv_path=None):\n",
    "    \"\"\"\n",
    "    Log experiment results to CSV for comparison across different runs\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object with experiment settings\n",
    "        model: Trained model\n",
    "        accuracy: Final accuracy achieved\n",
    "        train_time: Total training time in seconds\n",
    "        logger: TrainingLogger instance\n",
    "        csv_path: Path to CSV file for saving results\n",
    "    \"\"\"\n",
    "    if csv_path is None:\n",
    "        csv_path = config.experiment_csv\n",
    "\n",
    "    # Create experiments directory if it doesn't exist\n",
    "    #os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    directory = os.path.dirname(csv_path)\n",
    "    if directory:  # Only create if there's actually a directory path\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        print(f\"Ensuring directory exists: {directory}\")\n",
    "        \n",
    "    # Create run info dictionary\n",
    "    run_info = {\n",
    "        # Run identifiers\n",
    "        \"run_id\": logger.run_id,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
    "        \"experiment_name\": config.experiment_name,\n",
    "        \n",
    "        # Model architecture\n",
    "        \"backbone\": config.backbone,\n",
    "        \"dropout_rate\": config.dropout_rate,\n",
    "        \"freeze_backbone\": config.freeze_backbone,\n",
    "        \n",
    "        # Learning rate settings\n",
    "        \"lr_strategy\": config.lr_strategy,\n",
    "        \"fixed_lr\": config.fixed_lr if config.lr_strategy == \"fixed\" else None,\n",
    "        \"cyclic_base_lr\": config.cyclic_base_lr if config.lr_strategy == \"cyclic\" else None,\n",
    "        \"cyclic_max_lr\": config.cyclic_max_lr if config.lr_strategy == \"cyclic\" else None,\n",
    "        \n",
    "        # Training settings\n",
    "        \"epochs\": config.num_epochs,\n",
    "        \"batch_size\": config.batch_size,\n",
    "        \"image_size\": f\"{config.image_size[0]}x{config.image_size[1]}\",\n",
    "        \"augmentation\": \"Yes\" if config.use_augmentation else \"No\",\n",
    "        \"device\": str(config.device),\n",
    "        \n",
    "        # Performance metrics\n",
    "        \"final_accuracy\": round(accuracy, 2),\n",
    "        \"best_accuracy\": round(max(logger.val_accuracies), 2),\n",
    "        \"final_train_loss\": round(logger.train_losses[-1], 4),\n",
    "        \"final_val_loss\": round(logger.val_losses[-1], 4),\n",
    "        \"total_train_time_min\": round(train_time / 60, 2),\n",
    "        \"avg_epoch_time_sec\": round(sum(logger.epoch_times) / len(logger.epoch_times), 2),\n",
    "        \n",
    "        # Model metrics\n",
    "        \"model_size_mb\": round(model.get_model_size(), 2),\n",
    "        \"total_params\": model.get_parameter_count()\n",
    "    }\n",
    "    \n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_path, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=run_info.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(run_info)\n",
    "    \n",
    "    print(f\"‚úÖ Run info logged to: {csv_path}\")\n",
    "    \n",
    "    # Also save per-epoch metrics to a separate CSV for learning curves\n",
    "    epoch_data = {\n",
    "        \"run_id\": [logger.run_id] * len(logger.train_losses),\n",
    "        \"experiment_name\": [config.experiment_name] * len(logger.train_losses),\n",
    "        \"backbone\": [config.backbone] * len(logger.train_losses),\n",
    "        \"epoch\": list(range(1, len(logger.train_losses) + 1)),\n",
    "        \"train_loss\": logger.train_losses,\n",
    "        \"val_loss\": logger.val_losses,\n",
    "        \"val_accuracy\": logger.val_accuracies,\n",
    "        \"learning_rate\": logger.learning_rates,\n",
    "        \"epoch_time\": logger.epoch_times\n",
    "    }\n",
    "    \n",
    "    epoch_csv_path = os.path.join(config.experiments_dir, f\"epoch_metrics_{logger.run_id}.csv\")\n",
    "    pd.DataFrame(epoch_data).to_csv(epoch_csv_path, index=False)\n",
    "    print(f\"‚úÖ Epoch metrics saved to: {epoch_csv_path}\")\n",
    "\n",
    "def compare_runs(csv_path=\"training_runs.csv\"):\n",
    "    \"\"\"\n",
    "    Compare metrics across different training runs\n",
    "    \"\"\"\n",
    "    # Use config's experiment_csv if no path provided\n",
    "    if csv_path is None:\n",
    "        csv_path = config.experiment_csv\n",
    "\n",
    "    # Load the CSV\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"No run data found at {csv_path}\")\n",
    "        return\n",
    "        \n",
    "    runs_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if len(runs_df) < 2:\n",
    "        print(\"Need at least 2 runs to compare\")\n",
    "        return\n",
    "    \n",
    "    # Display the comparison table\n",
    "    print(\"\\nüìä COMPARISON OF TRAINING RUNS\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Select key columns to display, now including device\n",
    "    display_cols = [\"run_id\", \"backbone\", \"final_accuracy\", \"model_size_mb\", \n",
    "                   \"total_train_time_min\", \"augmentation\", \"lr_strategy\", \"device\", \"epochs\"]\n",
    "    \n",
    "    # Show the data\n",
    "    display(runs_df[display_cols].sort_values(\"final_accuracy\", ascending=False))\n",
    "    \n",
    "    # Plot comparisons\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy by backbone\n",
    "    sns.barplot(x=\"backbone\", y=\"final_accuracy\", hue=\"device\", data=runs_df, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Accuracy by Backbone (GPU vs CPU)\")\n",
    "    axes[0, 0].set_ylabel(\"Accuracy (%)\")\n",
    "    \n",
    "    # Training time by backbone\n",
    "    sns.barplot(x=\"backbone\", y=\"total_train_time_min\", hue=\"device\", data=runs_df, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Training Time by Backbone (GPU vs CPU)\")\n",
    "    axes[0, 1].set_ylabel(\"Time (minutes)\")\n",
    "    \n",
    "    # Model size by backbone\n",
    "    sns.barplot(x=\"backbone\", y=\"model_size_mb\", hue=\"device\", data=runs_df, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(\"Model Size by Backbone\")\n",
    "    axes[1, 0].set_ylabel(\"Size (MB)\")\n",
    "    \n",
    "    # Accuracy vs Model Size scatter plot\n",
    "    sns.scatterplot(x=\"model_size_mb\", y=\"final_accuracy\", hue=\"backbone\", \n",
    "                    style=\"device\", size=\"total_train_time_min\", sizes=(50, 200), \n",
    "                    data=runs_df, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(\"Accuracy vs. Model Size\")\n",
    "    axes[1, 1].set_xlabel(\"Model Size (MB)\")\n",
    "    axes[1, 1].set_ylabel(\"Accuracy (%)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional plot: Device comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    device_grouped = runs_df.groupby(['device', 'backbone']).agg({\n",
    "        'final_accuracy': 'mean',\n",
    "        'total_train_time_min': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    sns.barplot(x='backbone', y='total_train_time_min', hue='device', data=device_grouped)\n",
    "    plt.title('Average Training Time by Device and Backbone')\n",
    "    plt.ylabel('Training Time (minutes)')\n",
    "    plt.xlabel('Backbone Architecture')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a figure to visualize learning curves if epoch data is available\n",
    "    try:\n",
    "        # See if any epoch data files exist\n",
    "        epoch_files = [f for f in os.listdir() if f.startswith(\"epoch_metrics_\") and f.endswith(\".csv\")]\n",
    "        if not epoch_files:\n",
    "            return\n",
    "            \n",
    "        # Combine epoch data from different runs\n",
    "        all_epochs_df = pd.concat([pd.read_csv(f) for f in epoch_files])\n",
    "        \n",
    "        # Merge with run metadata\n",
    "        runs_df_simple = runs_df[[\"run_id\", \"backbone\", \"experiment_name\", \"device\"]]\n",
    "        merged_df = pd.merge(all_epochs_df, runs_df_simple, on=\"run_id\")\n",
    "        \n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.lineplot(x=\"epoch\", y=\"val_accuracy\", hue=\"experiment_name\", \n",
    "                    style=\"device\", data=merged_df)\n",
    "        plt.title(\"Validation Accuracy Across Runs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot learning curves: {e}\")\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: DataLoader for training set\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler (can be None)\n",
    "        device: Device to train on (cpu/cuda)\n",
    "        \n",
    "    Returns:\n",
    "        average_loss: Average loss over the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate if using scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Calculate average loss\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        val_loader: DataLoader for validation set\n",
    "        criterion: Loss function\n",
    "        device: Device to validate on (cpu/cuda)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (validation_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            accuracy = 100 * correct / total\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{accuracy:.2f}%\"})\n",
    "    \n",
    "    # Calculate metrics\n",
    "    validation_loss = running_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return validation_loss, accuracy\n",
    "\n",
    "def get_current_lr(optimizer):\n",
    "    \"\"\"Get the current learning rate from the optimizer\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def visualize_epoch_loss_history(epoch_loss_csv=\"epoch_loss.csv\", run_info_csv=\"training_runs_v4.csv\"):\n",
    "    \"\"\"\n",
    "    Visualize epoch loss history across different runs\n",
    "    \n",
    "    Args:\n",
    "        epoch_loss_csv: Path to the epoch loss CSV file\n",
    "        run_info_csv: Path to the run information CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    if epoch_loss_csv is None:\n",
    "        epoch_loss_csv = os.path.join(config.experiments_dir, \"epoch_loss.csv\")\n",
    "    \n",
    "    if run_info_csv is None:\n",
    "        run_info_csv = config.experiment_csv\n",
    "\n",
    "    try:\n",
    "        # Load the data\n",
    "        loss_df = pd.read_csv(epoch_loss_csv)\n",
    "        runs_df = pd.read_csv(run_info_csv)\n",
    "        \n",
    "        # Merge to get run metadata\n",
    "        runs_df_subset = runs_df[['run_id', 'backbone', 'lr_strategy', 'device', 'experiment_name']]\n",
    "        merged_df = pd.merge(loss_df, runs_df_subset, on='run_id', how='left')\n",
    "        \n",
    "        # Plot loss curves by run\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.lineplot(data=merged_df, x='epoch', y='loss', hue='experiment_name', style='backbone')\n",
    "        plt.title('Training Loss by Epoch Across Runs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot comparison by backbone\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.lineplot(data=merged_df, x='epoch', y='loss', hue='backbone', style='device')\n",
    "        plt.title('Training Loss by Backbone and Device')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show final loss statistics\n",
    "        print(\"\\nüìä Final Epoch Loss by Run:\")\n",
    "        last_epochs = merged_df.groupby('run_id').apply(lambda x: x.nlargest(1, 'epoch')).reset_index(drop=True)\n",
    "        display(last_epochs[['run_id', 'experiment_name', 'backbone', 'epoch', 'loss']].sort_values('loss'))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing epoch loss: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57801d",
   "metadata": {},
   "source": [
    "## üöÄ Training Process\n",
    "\n",
    "Now we'll train the model using the configured settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# Create criterion, optimizer and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                      lr=(config.fixed_lr if config.lr_strategy == \"fixed\" \n",
    "                         else config.cyclic_max_lr))\n",
    "\n",
    "if config.lr_strategy == \"cyclic\":\n",
    "    print(f\"üîÑ Using Cyclic LR: {config.cyclic_base_lr} ‚Üí {config.cyclic_max_lr} (step size: {config.cyclic_step_size})\")\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "        optimizer,\n",
    "        base_lr=config.cyclic_base_lr,\n",
    "        max_lr=config.cyclic_max_lr,\n",
    "        step_size_up=config.cyclic_step_size,\n",
    "        mode='triangular2',\n",
    "        cycle_momentum=False\n",
    "    )\n",
    "else:\n",
    "    print(f\"üìä Using fixed learning rate: {config.fixed_lr}\")\n",
    "    scheduler = None\n",
    "\n",
    "# Create training logger\n",
    "logger = TrainingLogger(os.path.join(config.log_dir, config.experiment_name))\n",
    "\n",
    "# Training loop\n",
    "best_accuracy = 0.0\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    print(f\"\\nüìå Epoch {epoch}/{config.num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, config.device)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = validate(model, test_loader, criterion, config.device)\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = get_current_lr(optimizer)\n",
    "    \n",
    "    # Log metrics\n",
    "    logger.log_epoch(epoch, train_loss, val_loss, val_accuracy, current_lr, epoch_time)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"üìä Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save checkpoint if best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        logger.save_checkpoint(model, optimizer, epoch, val_accuracy)\n",
    "    \n",
    "    # Visualize progress every few epochs\n",
    "    if epoch % max(1, config.num_epochs // 5) == 0 or epoch == config.num_epochs:\n",
    "        logger.plot_metrics()\n",
    "\n",
    "# Print total training time\n",
    "total_training_time = time.time() - total_start_time\n",
    "print(f\"\\n‚úÖ Training completed in {total_training_time/60:.2f} minutes\")\n",
    "print(f\"‚úÖ Best validation accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8451ba",
   "metadata": {},
   "source": [
    "## üìä Results Visualization\n",
    "\n",
    "Let's visualize the training results and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1102aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.plot_metrics()\n",
    "\n",
    "# Print training summary\n",
    "logger.summary()\n",
    "\n",
    "print(\"Available config attributes:\", dir(config))\n",
    "print(\"experiment_csv attribute:\", hasattr(config, \"experiment_csv\"))\n",
    "if hasattr(config, \"experiment_csv\"):\n",
    "    print(\"Value:\", config.experiment_csv)\n",
    "    \n",
    "# Log the experiment to CSV\n",
    "log_run_to_csv(config, model, best_accuracy, total_training_time, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b78243",
   "metadata": {},
   "source": [
    "## üìà Model Evaluation\n",
    "\n",
    "Now we'll evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "def get_predictions(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Get model predictions for all images in a data loader\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        data_loader: DataLoader to iterate through\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (true_labels, predictions, probabilities)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Getting predictions\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store results\n",
    "            all_probs.append(probabilities.cpu())\n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate batch results\n",
    "    all_probs = torch.cat(all_probs, dim=0)\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_labels.numpy(), all_preds.numpy(), all_probs.numpy()\n",
    "\n",
    "def generate_classification_report(true_labels, predictions, breed_labels):\n",
    "    \"\"\"\n",
    "    Generate a detailed classification report\n",
    "    \n",
    "    Args:\n",
    "        true_labels: Ground truth labels\n",
    "        predictions: Model predictions\n",
    "        breed_labels: List of breed class names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Classification metrics by breed\n",
    "    \"\"\"\n",
    "    # Find unique classes in the data\n",
    "    unique_classes = np.unique(np.concatenate((true_labels, predictions)))\n",
    "    \n",
    "    # Create formatted class names only for classes that appear in the data\n",
    "    labels = unique_classes\n",
    "    formatted_breeds = [breed_labels[label].split('-')[1] if '-' in breed_labels[label] else breed_labels[label] \n",
    "                       for label in labels]\n",
    "    \n",
    "    # Get classification report as dictionary\n",
    "    report = classification_report(true_labels, predictions, \n",
    "                                  labels=labels,  # Explicitly specify the labels\n",
    "                                  target_names=formatted_breeds, \n",
    "                                  output_dict=True,\n",
    "                                  zero_division=0)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Drop the unnecessary columns\n",
    "    if 'accuracy' in df.index:\n",
    "        df = df.drop('accuracy')\n",
    "    \n",
    "    # Sort by F1-score\n",
    "    df = df.sort_values('f1-score', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def visualize_confusion_matrix(true_labels, predictions, breed_labels, top_n=20, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Visualize the confusion matrix for top N breeds\n",
    "    \n",
    "    Args:\n",
    "        true_labels: Ground truth labels\n",
    "        predictions: Model predictions\n",
    "        breed_labels: List of breed class names\n",
    "        top_n: Number of top breeds to include\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Get formatted breed names\n",
    "    formatted_breeds = [breed.split('-')[1] if '-' in breed else breed for breed in breed_labels]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Find the top N breeds with most samples\n",
    "    breed_counts = np.sum(cm, axis=1)\n",
    "    top_indices = np.argsort(-breed_counts)[:top_n]\n",
    "    \n",
    "\n",
    "    # Extract submatrix for top breeds\n",
    "    cm_subset = cm[top_indices, :][:, top_indices]\n",
    "    breed_subset = [formatted_breeds[i] for i in top_indices]\n",
    "\n",
    "    # Normalize safely to avoid division by zero\n",
    "    row_sums = cm_subset.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  # Prevent divide-by-zero\n",
    "    cm_norm = cm_subset.astype('float') / row_sums\n",
    "\n",
    "    \n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "              xticklabels=breed_subset, yticklabels=breed_subset)\n",
    "    plt.title(f'Confusion Matrix (Top {top_n} Breeds)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get predictions\n",
    "true_labels, predictions, probabilities = get_predictions(model, test_loader, config.device)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(true_labels == predictions) * 100\n",
    "print(f\"Model accuracy on test set: {accuracy:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "report_df = generate_classification_report(true_labels, predictions, breed_labels)\n",
    "display(report_df.head(10))  # Show top 10 breeds\n",
    "\n",
    "# Visualize confusion matrix\n",
    "visualize_confusion_matrix(true_labels, predictions, breed_labels, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfcf77",
   "metadata": {},
   "source": [
    "## üîç Inference Examples\n",
    "\n",
    "Let's test our model on a few sample images to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337215d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on Sample Images\n",
    "\n",
    "def create_breed_prediction_grid(model, test_loader, breed_labels, device, num_samples=9, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Create a grid visualization of model predictions on test data\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        test_loader: DataLoader for test dataset\n",
    "        breed_labels: List of breed labels\n",
    "        device: Device to run inference on\n",
    "        num_samples: Number of samples to visualize in grid\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    # Get samples from test loader\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            batch_size = images.size(0)\n",
    "            all_images.append(images)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images.to(device))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            \n",
    "            if len(all_images) * batch_size >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_images = torch.cat(all_images)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    \n",
    "    # Select random indices\n",
    "    indices = torch.randperm(len(all_images))[:num_samples]\n",
    "    images = all_images[indices]\n",
    "    labels = all_labels[indices]\n",
    "    preds = all_preds[indices]\n",
    "    \n",
    "    # Create a grid of images\n",
    "    rows = int(np.sqrt(num_samples))\n",
    "    cols = int(np.ceil(num_samples / rows))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, label, pred) in enumerate(zip(images, labels, preds)):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        # Denormalize the image\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Get breed names\n",
    "        true_breed = breed_labels[label].split('-')[1] if '-' in breed_labels[label] else breed_labels[label]\n",
    "        pred_breed = breed_labels[pred].split('-')[1] if '-' in breed_labels[pred] else breed_labels[pred]\n",
    "        \n",
    "        # Display the image\n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Add colored title based on prediction correctness\n",
    "        color = 'green' if label == pred else 'red'\n",
    "        axes[i].set_title(f\"True: {true_breed}\\nPred: {pred_breed}\", color=color)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Turn off any unused subplots\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Model Predictions on Test Samples\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print accuracy on these samples\n",
    "    accuracy = (labels == preds).sum().item() / len(labels) * 100\n",
    "    print(f\"Sample Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Create prediction grid\n",
    "create_breed_prediction_grid(model, test_loader, breed_labels, config.device, num_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066e3a7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Performance Metrics\n",
    "\n",
    "Let's measure the inference speed and other performance metrics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "\n",
    "def measure_inference_time(model, input_size=(3, 224, 224), device=\"cpu\", num_warmup=10, num_runs=100):\n",
    "    \"\"\"\n",
    "    Measure inference time of the model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_size: Input tensor size (channels, height, width)\n",
    "        device: Device to run inference on\n",
    "        num_warmup: Number of warmup runs\n",
    "        num_runs: Number of timed runs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with inference metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, *input_size, device=device)\n",
    "    \n",
    "    # Warmup runs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmup):\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    # Timed runs\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(dummy_input)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_time = end_time - start_time\n",
    "    avg_time = total_time / num_runs\n",
    "    fps = 1.0 / avg_time\n",
    "    \n",
    "    return {\n",
    "        \"avg_inference_time_ms\": avg_time * 1000,\n",
    "        \"fps\": fps,\n",
    "        \"total_time_s\": total_time,\n",
    "        \"runs\": num_runs\n",
    "    }\n",
    "\n",
    "# Measure inference time\n",
    "performance = measure_inference_time(model, input_size=(3, *config.image_size), device=config.device)\n",
    "\n",
    "print(\"\\nüìä Performance Metrics:\")\n",
    "print(f\"üìè Model Size: {model.get_model_size():.2f} MB\")\n",
    "print(f\"üî¢ Total Parameters: {model.get_parameter_count():,}\")\n",
    "print(f\"‚ö° Avg. Inference Time: {performance['avg_inference_time_ms']:.2f} ms\")\n",
    "print(f\"üöÄ Frames Per Second: {performance['fps']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e479c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare previous runs if available\n",
    "try:\n",
    "    compare_runs()\n",
    "    visualize_epoch_loss_history()\n",
    "except Exception as e:\n",
    "    print(f\"Could not compare runs: {e}\")\n",
    "    print(\"This is probably your first run or the experiment CSV file doesn't exist yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb55644",
   "metadata": {},
   "source": [
    "## üíæ Save Final Model\n",
    "\n",
    "Finally, let's save our trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "final_model_path = os.path.join(config.log_dir, f\"{config.experiment_name}_final.pth\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'backbone': config.backbone,\n",
    "        'num_classes': len(breed_labels),\n",
    "        'dropout_rate': config.dropout_rate\n",
    "    },\n",
    "    'accuracy': accuracy,\n",
    "    'breed_labels': breed_labels\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4acc87",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "In this notebook, we've built and trained a dog breed classifier using transfer learning. Our model can identify dog breeds with high accuracy.\n",
    "\n",
    "### Key Accomplishments\n",
    "- Explored and processed the Stanford Dogs Dataset\n",
    "- Implemented data augmentation for better generalization\n",
    "- Created a flexible model architecture with multiple backbone options\n",
    "- Trained the model with optimal hyperparameters\n",
    "- Evaluated performance with detailed metrics\n",
    "- Created a system for inference on new images\n",
    "\n",
    "### Next Steps\n",
    "- Try ensemble methods for better accuracy\n",
    "- Implement model quantization for faster inference\n",
    "- Create a web interface for user uploads\n",
    "- Expand to more breeds or related tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4902491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä TRAINING SUMMARY (Run ID: {logger.run_id})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total training time: {total_training_time/60:.2f} minutes\")\n",
    "print(f\"Total epochs: {config.num_epochs}\")\n",
    "print(f\"Average epoch time: {sum(logger.epoch_times)/len(logger.epoch_times):.2f} seconds\")\n",
    "print(f\"Best validation accuracy: {max(logger.val_accuracies):.2f}%\")\n",
    "print(f\"Final validation accuracy: {logger.val_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final training loss: {logger.train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {logger.val_losses[-1]:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
